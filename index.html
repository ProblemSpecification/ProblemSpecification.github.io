<!DOCTYPE HTML>
<!--
	Fractal by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Michael Dennis</title>
		<link rel="icon" href="images/favicon.ico" type="image/x-icon">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">

				<div class="image"><img src="images/profile.jpeg" alt="" /></div>
				<div class="content">
					<div class="boarder">
						<!-- TODO figure out a way to include break out sections for current job-title and research interests -->
					<h1 style = "text-align: center"><a href="#">Michael Dennis</a></h1>
							Real world environments are complicated, too complicated to be completely
							specified in any simulation or model. I research <b> Problem Speceification </b> -- studying the boundry between what
							complexity must be described, and what can be artificially generated.
							<br>
							<br>
							To this end we have formalized the problem of
							<a href="https://arxiv.org/abs/2012.02096" target="_blank" rel="noopener noreferrer">Unsupervised Environment Design (UED)</a>,
							 which aims to build complex and challenging environments automaitcally to
							 promote
							 <a href="https://arxiv.org/abs/2203.01302" target="_blank" rel="noopener noreferrer">effient learning
								 </a> and
								 <a href="https://arxiv.org/abs/2110.02439" target="_blank" rel="noopener noreferrer">transfer</a> .  This framework has deep connections to
							 decision theory, which allow us to make gurantees about how the resulting policies
							 would preform in huamn-designed environments, without having ever trained on them.
							<br>
							<br>
							I am currently a final year PhD candidate at
							UC Berkeley's Center for
							 Human Compatible AI(<a href="humancompatible.ai" target="_blank" rel="noopener noreferrer">CHAI</a>) advised by Stuart Russell. I also have an
							 extensive background in computer science theory and <a href="https://arxiv.org/abs/1711.00068" target="_blank" rel="noopener noreferrer">computational geometry</a>.
							<br>
							<br>
							<br>
					</div>
					<ul class="icons">
						<li><a href="mailto:michael_dennis@cs.berkeley.edu" target="_blank" rel="noopener noreferrer" class="fas fa-envelope fa-xl"><span class="label">Email</span></a></li>
						<li><a href="https://twitter.com/MichaelD1729" target="_blank" rel="noopener noreferrer" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://scholar.google.com/citations?user=WXXu26AAAAAJ&hl=en&authuser=1" target="_blank" rel="noopener noreferrer" class="fas fa-graduation-cap big-icon"><span class="label">Google Scholar</span></a></li>
						<li><a href="https://github.com/MichaelDDennis" target="_blank" rel="noopener noreferrer" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<!-- TODO make CV and set here to download <li><a href="https://github.com/MichaelDDennis" class="fas fa-download big-icon"><span class="label">CV</span></a></li> -->
					</ul>
				<!--<div class="image phone"> -->
				<!--<div><div class="inner"><img src="images/screen.jpg" alt="" /></div></div> -->
			</div>
			</header>

		<!-- Two -->
			<section id="two" class="wrapper">
				<header class="major">
					<!--
					/* TODO incude cite link for each paper */
				-->
					<h2>Selected Papers</h2>
				</header>
				<div class="inner alt">
					<section class="spotlight">
						<div class="image"><img src="images/regret_s3.png" alt="" /></div>
						<div class="content">
							<h3 style = "text-align: center">PAIRED</h3>
							<b> NeurIPS 2020 Oral (top 1% of submissions) </b>
							<p>
								We formally define Unsupervised Environment Design (UED), as the problem of
								automatically building environments to promote learning and transfer.
								We show that UED has deep connections to the field of decisions under ignorance,
								and, borrowing from that field, aim to find high regret environments to promote efficent
								learning and transfer.
								<br>
								<br>
								Protagonist Antagonist Induced Regret Environment Design (PAIRED) provably finds
								minimax regret policies in equalibrium by training an adversary to geneterate
								levels which are diffuclt for the protagonist agent but easy for an antagonist agent.
								<br>
								<br>
								This motivates the adversary to <b>build difficult but solveable levels</b>,
								like the maze to the left, as an unsolveable maze is difficult for the antagonist.


							</p>

							<div class="authors">
								<b>Michael Dennis*</b>,
								<a href="https://natashajaques.ai/" target="_blank" rel="noopener noreferrer"> Natasha Jaques*</a>,
								<a href="https://eugenevinitsky.github.io/" target="_blank" rel="noopener noreferrer"> Eugene Vinitsky</a>,
								<a href="https://bayen.berkeley.edu/alex-bayen" target="_blank" rel="noopener noreferrer"> Alexandre Bayen</a>,
							  <a href="https://people.eecs.berkeley.edu/~russell/" target="_blank" rel="noopener noreferrer"> Stuart Russell</a>,
								Andrew Critch,
								<a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank" rel="noopener noreferrer"> Sergey Levine</a>
							</div>

							<div class="btn-links">
								Resources:
							<a class="btn btn-outline-primary btn-page-header btn-sm"
							href="https://arxiv.org/abs/2012.02096" target="_blank" rel="noopener">
							PDF
							</a>

							<!---<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename="/publication/paired/cite.bib">
							  Cite
							</a> -->

							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://www.youtube.com/channel/UCI6dkF8eNrCz6XiBJlV9fmw/videos"
								target="_blank" rel="noopener">
						    Demo Videos
						  </a>

							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://github.com/google-research/google-research/tree/master/social_rl/adversarial_env"
								target="_blank" rel="noopener">
								Code (Tensorflow)
							</a>

							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://github.com/ucl-dark/paired"
								target="_blank" rel="noopener">
								Code (Pytorch)
							</a>

						</div>

						<div class="btn-links">
							Reception:
						  <a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://neurips.cc/virtual/2020/public/poster_985e9a46e10005356bbaf194249f6856.html"
								target="_blank" rel="noopener">
						    NeurIPS Oral
						  </a>

						  <a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://www.sciencemag.org/news/2021/01/who-needs-teacher-artificial-intelligence-designs-lesson-plans-itself"
								target="_blank" rel="noopener">
						    Science article
						  </a>

						  <a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://ai.googleblog.com/2021/03/paired-new-multi-agent-approach-for.html"
								target="_blank" rel="noopener">
						    Google AI Blog
						  </a>

							  </div>
						</div>
					</section>
					<section class="spotlight">
						<div class="image"><img src="images/a_policies_snapshot.png" alt="" /></div>
						<div class="content">
							<h3 style = "text-align: center">Adversarial Policies</h3>
							<b> ICLR 2020 </b> <br>
							<b> NeurIPS 2019 DeepRL Workshop Talk (top 6% of accepted papers)  </b><br>
							<b> ICML 2019 SPML Workshop Spotlight (top 6% of accepted papers)  </b> <br>

							<p>
								Deep reinforcement learning (RL) policies are known to be vulnerable to adversarial perturbations to their observations, similar to adversarial examples for classifiers.
								However, an attacker is not usually able to directly modify another agent’s observations. This might lead one to wonder: is it possible to attack an RL agent simply
								by choosing an adversarial policy acting in a multi-agent environment so as to
								create natural observations that are adversarial? <br><br>

								 We demonstrate the existence of
								adversarial policies in zero-sum games between simulated humanoid robots, against
								 state-of-the-art victims. As
	 							shown in our <a
							 href="https://adversarialpolicies.github.io/" target="_blank" rel="noopener">demo videos</a>, the adversarial policies
								 reliably and dramatically win against the victims with seemingly random and uncoordinated behavior.
							</p>

							<div class="authors">
								<a href="https://www.gleave.me/" target="_blank" rel="noopener noreferrer"> Adam Gleave</a>,
								<b>Michael Dennis</b>
								<a href="http://decodyng.com/" target="_blank" rel="noopener noreferrer"> Cody Wild</a>,
								<a href="https://scholar.google.com/citations?user=eSgXTkkAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"> Neel Kant</a>,
								<a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank" rel="noopener noreferrer"> Sergey Levine</a>,
								<a href="https://people.eecs.berkeley.edu/~russell/" target="_blank" rel="noopener noreferrer"> Stuart Russell</a>
							</div>

							<div style="text-align:left"  class="btn-links">
								Resources:
							<a class="btn btn-outline-primary btn-page-header btn-sm"
							href="https://arxiv.org/abs/1905.10615" target="_blank" rel="noopener">
							PDF
							</a>

							<!---<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename="/publication/paired/cite.bib">
								Cite
							</a> -->

							<a class="btn btn-outline-primary btn-page-header btn-sm"
							href="https://adversarialpolicies.github.io/" target="_blank" rel="noopener">
							Demo Videos
							</a>

							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://github.com/humancompatibleai/adversarial-policies"
								target="_blank" rel="noopener">
								Code
							</a>

						</div>

						<div style="text-align:left"   class="btn-links">
							Reception:
							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://iclr.cc/virtual_2020/poster_HJgEMpVFwB.html"
								target="_blank" rel="noopener">
								ICLR
							</a>

							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://slideslive.com/38922702/contributed-talk-adversarial-policies-attacking-deep-reinforcement-learning?ref=folder-70324"
								target="_blank" rel="noopener">
								NeurIPS DeepRL Talk
							</a>

							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=3526#wse-detail-5351"
								target="_blank" rel="noopener">
								ICML SPML Spotlight
							</a>

							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://www.science.org/content/article/watch-ai-goalie-psych-out-its-opponent-most-hilarious-way"
								target="_blank" rel="noopener">
								Science article
							</a>

							<a class="btn btn-outline-primary btn-page-header btn-sm"
								href="https://bair.berkeley.edu/blog/2020/03/27/attacks/"
								target="_blank" rel="noopener">
								BAIR Blog
							</a>

						</div>
					</section>
					<!-- On track for ICML, still needs code-->
					<section class="spotlight">
						<div class="image"><img src="images/ACCELL_Spotlight.png" alt="" /></div>
						<div class="content">
							<h3 style = "text-align: center">ACCELL</h3>
							<b>NeurIPS 2021 DeepRL Workshop</b>

							<p>
									Two important insights in Unsupervised Environment Design (UED) are:
									<ul>
										<li> <b>High Regret Levels</b> promote efficent learning and transfer  (See <a href="https://arxiv.org/abs/2012.02096" target="_blank" rel="noopener noreferrer">PAIRED</a>, <a href="https://arxiv.org/abs/2110.02439"target="_blank" rel="noopener noreferrer">Robust PLR</a>)</li>
										<li> <b>Evolution</b> is more efficent at optimizing environments than RL  (See <a href="https://arxiv.org/abs/1901.01753" target="_blank" rel="noopener noreferrer">POET</a>) </li>
									</ul>
									We combine these two threads, using currating level edits to maximize regret, and allowing evolution to compound this effect over time. </br>

									<b>ACCELL achieves state of the art performance in every domain tested</b> including:
									<ul>
									 <li> Bipedal Walker (used in POET)</li>
									 <li> Minigrid and Car Racing environments (used in PAIRED and Robust PLR) </li>
									</ul>

									Stress test our method yourself with our <a
									href="https://accelagent.github.io/" target="_blank" rel="noopener"> interactive demo right in your browser</a>!


							</p>

							<div class="authors">
								<a href="https://t.co/lQPsq7NlQ4" target="_blank" rel="noopener noreferrer"> Jack Parker-Holder</a>,
								<a href="https://minch.co/" target="_blank" rel="noopener noreferrer">  Minqi Jiang</a>,
								<b>Michael Dennis</b>
								<a href="https://www.samvelyan.com/" target="_blank" rel="noopener noreferrer">  Mikayel Samvelyan</a>,
								<a href="https://www.jakobfoerster.com/" target="_blank" rel="noopener noreferrer"> Jakob Foerster </a>,
								<a href="https://www.egrefen.com/" target="_blank" rel="noopener noreferrer"> Edward Grefenstette</a>,
								<a href="https://rockt.github.io/" target="_blank" rel="noopener noreferrer"> Tim Rocktäschel</a>
							</div>

 						 					<div class="btn-links">
 						 						Resources:
 						 					<a class="btn btn-outline-primary btn-page-header btn-sm"
 						 					href="http://arxiv.org/abs/2203.01302" target="_blank" rel="noopener">
 						 					PDF
 						 					</a>

 						 					<!---<a href="#" class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename="/publication/paired/cite.bib">
 						 						Cite
 						 					</a> -->

 						 					<a class="btn btn-outline-primary btn-page-header btn-sm"
 						 					href="https://accelagent.github.io/" target="_blank" rel="noopener">
 						 					Interactive Demo
 						 					</a>

 						 				</div>

 						 				<div class="btn-links">
 						 					Reception:
 						 					<a class="btn btn-outline-primary btn-page-header btn-sm"
 						 						href="https://www.youtube.com/watch?v=povBDxUn1VQ&feature=youtu.be"
 						 						target="_blank" rel="noopener">
 						 						Youtube Discussion
 						 					</a>

 						 					<a class="btn btn-outline-primary btn-page-header btn-sm"
 						 						href="https://www.youtube.com/watch?v=16BsJI5I-Yw&feature=youtu.be"
 						 						target="_blank" rel="noopener">
 						 						Youtube Interview
 						 					</a>

 						 					<a class="btn btn-outline-primary btn-page-header btn-sm"
 						 						href="https://openreview.net/forum?id=3qGInPFqR0p"
 						 						target="_blank" rel="noopener">
 						 						NeurIPS DeepRL
 						 					</a>
 						 				</div>

						</div>


					</section>
					<!-- still needs code -->
					<section class="spotlight">
						<div class="image"><img src="images/track_ref_robustPLR.png" alt="" /></div>
						<div class="content">
							<h3 style = "text-align: center">Robust PLR</h3>
							<b> NeurIPS 2021 </b>

							<p>
								Unsupervised Environment Deesign (UED) is a promising technique to accelerate training and promote transfer performance.
								In this work we study two classes of technqies for UED, <b>active design</b> like PAIRED and <b>passive curration</b> like PLR, casting them into one framework of Duel Curriculem Design (DCD).
								The DCD allows us to extend regret-based gurantees to combinations of existing frameworks.
								<br>
								<br>
								Moreover, the theory suggests a counterintuitive conclusion: <br>
								PLR can be improved by training on <b>less data</b>, which we call Robust PLR.<br>
								essentailly UED methods should focus on quality over quantity
								<br>
								<br>
								We validate Robust PLR emperically, showing it achieves state of the art in challenging transfer tasks.
							</p>

							<div class="authors">
								<a href="https://minch.co/" target="_blank" rel="noopener noreferrer">  Minqi Jiang*</a>,
								<b>Michael Dennis*</b>
								<a href="https://t.co/lQPsq7NlQ4" target="_blank" rel="noopener noreferrer"> Jack Parker-Holder</a>,
								<a href="https://www.jakobfoerster.com/" target="_blank" rel="noopener noreferrer"> Jakob Foerster </a>,
								<a href="https://www.egrefen.com/" target="_blank" rel="noopener noreferrer"> Edward Grefenstette</a>,
								<a href="https://rockt.github.io/" target="_blank" rel="noopener noreferrer"> Tim Rocktäschel</a>
							</div>

											<div style="text-align:left"  class="btn-links">
												Links:
											<a class="btn btn-outline-primary btn-page-header btn-sm"
											href="https://arxiv.org/abs/2110.02439" target="_blank" rel="noopener">
											PDF
											</a>
											<a class="btn btn-outline-primary btn-page-header btn-sm"
												href="https://slideslive.com/38968339/replayguided-adversarial-environment-design?ref=speaker-24021"
												target="_blank" rel="noopener">
												NeurIPS
											</a>
										</div>

						</div>
					</section>
					<!-- Not on Arxiv yet, on track for ICML, still needs code -->
					<section class="spotlight">
						<div class="image"><img src="images/samplr_spotlight.png" alt="" /></div>
						<div class="content">
							<h3 style = "text-align: center"> SAMPLR</h3>
							<b> NeurIPS 2021 DeepRL Workshop </b>
							<p>
							 	For a policy to archive good performance it often needs to internalize the probabilities of certain events which are aleatorically uncertain -- the chance the next card is an ace, the next coin flip turns up heads, or there is rain on the next day.
								However, curriculum techniques such as Unsupervised Environment Design (UED), which are often instrumenal to the policies performance, function precisely by changing these distributions durring training to focus on the most informative experiences, we call this effect curriculum induced covarient shfit (CICS).
								CICS can cause the agent to overestimate the probability of the rare events the curriculum prioritized, thus internalizing incorrect probabilities and making wrong decisions.
								SAMPELR corrects for this bias, without changing the curriculum distributions, by correcting the value-estimate to be unbiased with respect to the true distribution of aleatoric parameters.
								Thus we mainatain the ability of the curriculum to sample informative experience, while correcting the updates from that experience to correct for the CISC the curriculum introduced.
							</p>

						<div class="authors">
							<a href="https://minch.co/" target="_blank" rel="noopener noreferrer">  Minqi Jiang</a>,
							<b>Michael Dennis</b>
							<a href="https://t.co/lQPsq7NlQ4" target="_blank" rel="noopener noreferrer"> Jack Parker-Holder</a>,
							<a href="https://scholar.google.com/citations?user=I6aB-YUAAAAJ&hl=en" target="_blank" rel="noopener noreferrer"> Andrei Lupu</a>,
							<a href="https://heiner.ai/blog/" target="_blank" rel="noopener noreferrer"> Heinrich Kuttler</a>,
							<a href="https://www.egrefen.com/" target="_blank" rel="noopener noreferrer"> Edward Grefenstette</a>,
							<a href="https://rockt.github.io/" target="_blank" rel="noopener noreferrer"> Tim Rocktäschel</a>,
							<a href="https://www.jakobfoerster.com/" target="_blank" rel="noopener noreferrer"> Jakob Foerster </a>
						</div>

										<div class="btn-links">
											Links:
										<a class="btn btn-outline-primary btn-page-header btn-sm"
										href="https://openreview.net/pdf?id=wYqLTy4wkor" target="_blank" rel="noopener">
										PDF
										</a>
										<a class="btn btn-outline-primary btn-page-header btn-sm"
											href="https://slideslive.com/38971048/grounding-aleatoric-uncertainty-in-unsupervised-environment-design?ref=speaker-28796"
											target="_blank" rel="noopener">
											NeurIPS 2021 DeepRL
										</a>
									</div>
									</div>
					</section>
					<!--TODO should change picture -->
					<section class="spotlight">
						<div class="image"><img src="images/epic_spotlight.png" alt="" /></div>
						<div class="content">
							<h3 style = "text-align: center">EPIC</h3>
							<b> ICLR 2021 Spotlight (top 5% of submissions) </b>
							<p>
							  The evaluation of reward learning technqiues, such as IRL, have traditionally rellied on evaluating
								the behavior of a policy optimizing the learned reward.  This process is slow, fails when the policy optimiztion fails, and
							  restricts our evaulation to test environments.
								<br>
								<br>
								Equivalent-Policy Invariant Comparison (EPIC) distance evaluates reward functions directly, quickly, and reliably -- without requiring policy optimization.
								<br>
								<br>
								Moreover, EPIC is:
								<ul>
									<li>Invariant to reward shaping</li>
									<li>Invariant reward scaling</li>
									<li>Bounds the regret incured by a policy against the true reward function</li>
									<li>Invariant to dynamics, so regret bound applies to all enviroments</li>
								</ul>

								</p>

						<div class="authors">
							<a href="https://www.gleave.me/" target="_blank" rel="noopener noreferrer">  Adam Gleave</a>,
							<b>Michael Dennis</b>
							<a href="https://en.wikipedia.org/wiki/Shane_Legg" target="_blank" rel="noopener noreferrer"> Shane Legg</a>,
							<a href="https://people.eecs.berkeley.edu/~russell/" target="_blank" rel="noopener noreferrer">  Stuart Russell</a>,
							<a href="https://jan.leike.name/" target="_blank" rel="noopener noreferrer"> Jan Leike</a>
						</div>

										<div style="text-align:left"  class="btn-links">
											Resources:
										<a class="btn btn-outline-primary btn-page-header btn-sm"
										href="https://arxiv.org/abs/2006.13900" target="_blank" rel="noopener">
										PDF
										</a>
										<a class="btn btn-outline-primary btn-page-header btn-sm"
										href="https://github.com/HumanCompatibleAI/evaluating-rewards" target="_blank" rel="noopener">
										Code
										</a>
									</div>
									<div style="text-align:left"  class="btn-links">
										Reception:
										<a class="btn btn-outline-primary btn-page-header btn-sm"
											href="https://bair.berkeley.edu/blog/2021/04/20/epic/"
											target="_blank" rel="noopener">
											BAIR Blog
										</a>
										<a class="btn btn-outline-primary btn-page-header btn-sm"
											href="https://iclr.cc/virtual/2021/spotlight/3418"
											target="_blank" rel="noopener">
											ICLR 2021 Spotlight
										</a>
										<a class="btn btn-outline-primary btn-page-header btn-sm"
											href="https://slideslive.com/38940987/quantifying-differences-in-reward-functions?ref=account-folder-62083-folders"
											target="_blank" rel="noopener">
											NeurIPS 2020 DeepRL workshop
										</a>
									</div>
									</div>
					</section>

					<!-- eventually do sub-sections you can select at the top and
					make a section for coopoeration, include the following:
					A new formalism, method and open issues for zero-shot coordination
					Accumulating Risk Capital Through Investing in Cooperation
					-->

			</section>


		<!-- Footer -->
			<footer id="footer">
				<p class="copyright">Design Credits: <a href="https://html5up.net/fractal">HTML5 UP</a></p>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
